---
title: Slowly Changing Adversarial Bandit Algorithms are Efficient for Discounted
  MDPs
abstract: Reinforcement learning generalizes multi-armed bandit problems with additional
  difficulties of a longer planning horizon and unknown transition kernel. We explore
  a black-box reduction from discounted infinite-horizon tabular reinforcement learning
  to multi-armed bandits, where, specifically, an independent bandit learner is placed
  in each state. We show that, under ergodicity and fast mixing assumptions, any slowly
  changing adversarial bandit algorithm achieving optimal regret in the adversarial
  bandit setting can also attain optimal expected regret in infinite-horizon discounted
  Markov decision processes, with respect to the number of rounds $T$. Furthermore,
  we examine our reduction using a specific instance of the exponential-weight algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kash24a
month: 0
tex_title: Slowly Changing Adversarial Bandit Algorithms are Efficient for Discounted
  MDPs
firstpage: 683
lastpage: 718
page: 683-718
order: 683
cycles: false
bibtex_author: Kash, Ian A. and Reyzin, Lev and Yu, Zishun
author:
- given: Ian A.
  family: Kash
- given: Lev
  family: Reyzin
- given: Zishun
  family: Yu
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/kash24a/kash24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
