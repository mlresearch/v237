---
title: 'On the Sample Complexity of Two-Layer Networks: Lipschitz Vs. Element-Wise
  Lipschitz Activation'
abstract: 'This study delves into the sample complexity of two-layer neural networks.
  For a given reference matrix $W^0 \in \mathbb{R}^{\mathcal{T}\times d}$ (typically
  representing initial training weights) and an $O(1)$-Lipschitz activation function
  $\sigma:\mathbb{R}\to\mathbb{R}$, we examine the class $\mathcal{H}_{W^0, B, R,
  r}^{\sigma} = \left\{\textbf{x}\mapsto ⟨\textbf{v},\sigma((W+W^0)\textbf{x})⟩: \|W\|_{\text{Frobenius}}
  \le R, \|\textbf{v}\| \le r, \|\textbf{x}\|\le B\right\}$. We demonstrate that when
  $\sigma$ operates element-wise, the sample complexity of $\mathcal{H}_{W^0, B, R,
  r}^{\sigma}$ is bounded by $\tilde O \left(\frac{L^2 B^2 r^2 (R^2+\|W\|^2_{\text{Spectral}})}{\epsilon^2}\right)$.
  This bound is optimal, barring logarithmic factors, and depends logarithmically
  on the width $\mathcal{T}$. This finding builds upon [Vardi et al., 2022], who established
  a similar outcome for $W^0 = 0$. Our motivation stems from the real-world observation
  that trained weights often remain close to their initial counterparts, implying
  that $\|W\|_{\text{Frobenius}} \ll \|W+W^0\|_{\text{Frobenius}}$. To arrive at our
  conclusion, we employed and enhanced a recently new norm-based bounds method, the
  Approximate Description Length (ADL), as proposed by [Daniely and and Granot, 2019].
  Finally, our results underline the crucial role of the element-wise nature of $\sigma$
  for achieving a logarithmic width-dependent bound. To illustrate, we prove that
  there exists an $O(1)$-Lipschitz (non-element-wise) activation function $\Psi:\mathbb{R}^{\mathcal{T}}\to\mathbb{R}^{\mathcal{T}}$
  where the sample complexity of $\mathcal{H}_{W^0, B, R, r}^{\Psi}$ increases linearly
  with the width.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: daniely24a
month: 0
tex_title: 'On the Sample Complexity of Two-Layer Networks: Lipschitz Vs. Element-Wise
  Lipschitz Activation'
firstpage: 505
lastpage: 517
page: 505-517
order: 505
cycles: false
bibtex_author: Daniely, Amit and Granot, Elad
author:
- given: Amit
  family: Daniely
- given: Elad
  family: Granot
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/daniely24a/daniely24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
