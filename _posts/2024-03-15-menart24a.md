---
title: Differentially Private Non-Convex Optimization under the KL Condition with
  Optimal Rates
abstract: We study private empirical risk minimization (ERM) problem for losses satisfying
  the $(\gamma,\kappa)$-Kurdyka-{Ł}ojasiewicz (KL) condition, that is, the empirical
  loss $F$ satisfies $F(w)-\min_{w}F(w) \leq \gamma^\kappa \|\nabla F(w)\|^\kappa$.
  The Polyak-{Ł}ojasiewicz (PL) condition is a special case of this condition when
  $\kappa=2$. Specifically, we study this problem under the constraint of $\rho$ zero-concentrated
  differential privacy (zCDP). When $\kappa\in[1,2]$ and the loss function is Lipschitz
  and smooth over a sufficiently large region, we provide a new algorithm based on
  variance reduced gradient descent that achieves the rate $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$
  on the excess empirical risk, where $n$ is the dataset size and $d$ is the dimension.
  We further show that this rate is nearly optimal. When $\kappa \geq 2$ and the loss
  is instead Lipschitz and weakly convex, we show it is possible to achieve the rate
  $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^\kappa\big)$ with a private
  implementation of the proximal point method. When the KL parameters are unknown,
  we provide a novel modification and analysis of the noisy gradient descent algorithm
  and show that this algorithm achieves a rate of $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^{\frac{2\kappa}{4-\kappa}}\big)$
  adaptively, which is nearly optimal when $\kappa = 2$. We further show that, without
  assuming the KL condition, the same gradient descent algorithm can achieve fast
  convergence to a stationary point when the gradient stays sufficiently large during
  the run of the algorithm. Specifically, we show that this algorithm can approximate
  stationary points of Lipschitz, smooth (and possibly nonconvex) objectives with
  rate as fast as $\tilde{O}\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)$ and never worse
  than $\tilde{O}\big(\big(\frac{\sqrt{d}}{n\sqrt{\rho}}\big)^{1/2}\big)$. The latter
  rate matches the best known rate for methods that do not rely on variance reduction.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: menart24a
month: 0
tex_title: Differentially Private Non-Convex Optimization under the KL Condition with
  Optimal Rates
firstpage: 868
lastpage: 906
page: 868-906
order: 868
cycles: false
bibtex_author: Menart, Michael and Ullah, Enayat and Arora, Raman and Bassily, Raef
  and Guzman, Cristobal
author:
- given: Michael
  family: Menart
- given: Enayat
  family: Ullah
- given: Raman
  family: Arora
- given: Raef
  family: Bassily
- given: Cristobal
  family: Guzman
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/menart24a/menart24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
