---
title: Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples
abstract: 'We study the problem of estimating mixtures of Gaussians under the constraint
  of differential privacy (DP). Our main result is that $\text{poly}(k,d,1/\alpha,1/\varepsilon,\log(1/\delta))$
  samples are sufficient to estimate a mixture of $k$ Gaussians in $\mathbb{R}^d$
  up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP.
  This is the first finite sample complexity upper bound for the problem that does
  not make any structural assumptions on the GMMs. To solve the problem, we devise
  a new framework which may be useful for other tasks. On a high level, we show that
  if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits
  a “locally small” cover (Bun et al., 2021) with respect to total variation distance,
  then the class of its mixtures is privately learnable. The proof circumvents a known
  barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover
  (Aden-Ali et al., 2021b). '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: afzali24a
month: 0
tex_title: Mixtures of Gaussians are Privately Learnable with a Polynomial Number
  of Samples
firstpage: 47
lastpage: 73
page: 47-73
order: 47
cycles: false
bibtex_author: Afzali, Mohammad and Ashtiani, Hassan and Liaw, Christopher
author:
- given: Mohammad
  family: Afzali
- given: Hassan
  family: Ashtiani
- given: Christopher
  family: Liaw
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/afzali24a/afzali24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
