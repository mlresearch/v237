---
title: Near-continuous time Reinforcement Learning for continuous state-action spaces
abstract: We consider the reinforcement learning problem of controlling an unknown
  dynamical system to maximise the long-term average reward along a single trajectory.  Most
  of the literature considers system interactions that occur in discrete time and
  discrete state-action spaces.  Although this standpoint is suitable for games, it
  is often inadequate for systems in which interactions occur at a high frequency,
  if not in continuous time, or those whose state spaces are large if not inherently
  continuous.  Perhaps the only exception is the linear quadratic framework for which
  results exist both in discrete and continuous time.   However, its ability to handle
  continuous states comes with the drawback of a rigid dynamic and reward structure.
  This work aims to overcome these shortcomings by modelling interaction times with
  a Poisson clock of frequency $\varepsilon^{-1}$ which captures arbitrary time scales
  from discrete ($\varepsilon=1$) to continuous time ($\varepsilon\downarrow0$).  In
  addition, we consider a generic reward function and model the state dynamics according
  to a jump process with an arbitrary transition kernel on $\mathbb{R}^d$.  We show
  that the celebrated optimism protocol applies when the sub-tasks (learning and planning)
  can be performed effectively.  We tackle learning by extending the eluder dimension
  framework and propose an approximate planning method based on a diffusive limit
  ($\varepsilon\downarrow0$) approximation of the jump process. Overall, our algorithm
  enjoys a regret of order $\tilde{\mathcal{O}}(\sqrt{T})$ or $\tilde{\mathcal{O}}(\varepsilon^{1/2}
  T+\sqrt{T})$ with the approximate planning.  As the frequency of interactions blows
  up, the approximation error $\varepsilon^{1/2} T$ vanishes, showing that $\tilde{\mathcal{O}}(\sqrt{T})$
  is attainable in near-continuous time.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: croissant24a
month: 0
tex_title: Near-continuous time {Reinforcement} {Learning} for continuous state-action
  spaces
firstpage: 444
lastpage: 498
page: 444-498
order: 444
cycles: false
bibtex_author: Croissant, Lorenzo and Abeille, Marc and Bouchard, Bruno
author:
- given: Lorenzo
  family: Croissant
- given: Marc
  family: Abeille
- given: Bruno
  family: Bouchard
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/croissant24a/croissant24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
