---
title: The complexity of non-stationary reinforcement learning
abstract: 'The problem of continual learning in the domain of reinforcement learning,
  often called non-stationary reinforcement learning, has been identified as an important
  challenge to the application of reinforcement learning. We prove a worst-case complexity
  result, which we believe captures this challenge: Modifying the probabilities or
  the reward of a single state-action pair in a reinforcement learning problem requires
  an amount of time almost as large as the number of states  in order to keep the
  value function up to date, unless the strong exponential time hypothesis (SETH)
  is false; SETH is a widely accepted strengthening of the P $\neq$ NP conjecture.  Recall
  that the number of states in current applications of reinforcement learning is typically
  astronomical.  In contrast, we show that just adding a new state-action pair is
  considerably easier to implement. '
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: peng24a
month: 0
tex_title: The complexity of non-stationary reinforcement learning
firstpage: 972
lastpage: 996
page: 972-996
order: 972
cycles: false
bibtex_author: Peng, Binghui and Papadimitriou, Christos
author:
- given: Binghui
  family: Peng
- given: Christos
  family: Papadimitriou
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/peng24a/peng24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
