---
title: 'RedEx: Beyond Fixed Representation Methods via Convex Optimization'
abstract: Optimizing Neural networks is a difficult task which is still not well understood.
  On the other hand, fixed representation methods such as kernels and random features
  have provable optimization guarantees but inferior performance due to their inherent
  inability to learn the representations. In this paper, we aim at bridging this gap
  by presenting a novel architecture called RedEx (Reduced Expander Extractor) that
  is as expressive as neural networks and can also be trained in a layer-wise fashion
  via a convex program with semi-definite constraints and optimization guarantees.
  We also show that RedEx provably surpasses fixed representation methods, in the
  sense that it can efficiently learn a family of target functions which fixed representation
  methods cannot.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: daniely24b
month: 0
tex_title: 'RedEx: Beyond Fixed Representation Methods via Convex Optimization'
firstpage: 518
lastpage: 543
page: 518-543
order: 518
cycles: false
bibtex_author: Daniely, Amit and Schain, Mariano and Yehudai, Gilad
author:
- given: Amit
  family: Daniely
- given: Mariano
  family: Schain
- given: Gilad
  family: Yehudai
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/daniely24b/daniely24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
