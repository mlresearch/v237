---
title: Multiclass Learnability Does Not Imply Sample Compression
abstract: 'A hypothesis class admits a sample compression scheme, if for every sample
  labeled by a hypothesis from the class, it is possible to retain only a small subsample,
  using which  the labels on the entire sample can be inferred. The size of the compression
  scheme is an upper bound on the size of the subsample produced. Every learnable
  binary hypothesis class (which must necessarily have finite VC dimension) admits
  a sample compression scheme of size only a finite function of its VC dimension,
  independent of the sample size. For multiclass hypothesis classes, the analog of
  VC dimension is the DS dimension. We show that the analogous statement pertaining
  to sample compression is not true for multiclass hypothesis classes: every learnable
  multiclass hypothesis class, which must necessarily have finite DS dimension, does
  not admit a sample compression scheme of size only a finite function of its DS dimension.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: pabbaraju24a
month: 0
tex_title: Multiclass Learnability Does Not Imply Sample Compression
firstpage: 930
lastpage: 944
page: 930-944
order: 930
cycles: false
bibtex_author: Pabbaraju, Chirag
author:
- given: Chirag
  family: Pabbaraju
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/pabbaraju24a/pabbaraju24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
