---
title: Provable Accelerated Convergence of Nesterov’s Momentum for Deep ReLU Neural
  Networks
abstract: Current state-of-the-art analyses on the convergence of gradient descent
  for training neural networks focus on characterizing properties of the loss landscape,
  such as the Polyak-Lojaciewicz (PL) condition and the restricted strong convexity.
  While gradient descent converges linearly under such conditions, it remains an open
  question whether Nesterov’s momentum enjoys accelerated convergence under similar
  settings and assumptions. In this work, we consider a new class of objective functions,
  where only a subset of the parameters satisfies strong convexity, and show Nesterov’s
  momentum achieves acceleration in theory for this objective class. We provide two
  realizations of the problem class, one of which is deep ReLU networks, which constitutes
  this work as the first that proves an accelerated convergence rate for non-trivial
  neural network architectures.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liao24a
month: 0
tex_title: Provable Accelerated Convergence of Nesterov’s Momentum for Deep ReLU Neural
  Networks
firstpage: 732
lastpage: 784
page: 732-784
order: 732
cycles: false
bibtex_author: Liao, Fangshuo and Kyrillidis, Anastasios
author:
- given: Fangshuo
  family: Liao
- given: Anastasios
  family: Kyrillidis
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/liao24a/liao24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
