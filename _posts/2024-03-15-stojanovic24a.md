---
title: Tight bounds for maximum $\ell_1$-margin classifiers
abstract: Popular iterative algorithms such as boosting methods and coordinate descent
  on linear models converge to the maximum $\ell_1$-margin classifier, a.k.a. sparse
  hard-margin SVM, in high dimensional regimes where the data is linearly separable.
  Previous works consistently show that many estimators relying on the $\ell_1$-norm
  achieve improved statistical rates for hard sparse ground truths. We show that surprisingly,
  this adaptivity does not apply to the maximum $\ell_1$-margin classifier for a standard
  discriminative setting. In particular, for the noiseless setting, we prove tight
  upper and lower bounds for the prediction error that match existing rates of order
  $\frac{\|\w^*\|_1^{2/3}}{n^{1/3}}$ for general ground truths. To complete the picture,
  we show that when interpolating noisy observations, the error vanishes at a rate
  of order $\frac{1}{\sqrt{\log(d/n)}}$. We are therefore first to show benign overfitting
  for the maximum $\ell_1$-margin classifier.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: stojanovic24a
month: 0
tex_title: Tight bounds for maximum $\ell_1$-margin classifiers
firstpage: 1055
lastpage: 1112
page: 1055-1112
order: 1055
cycles: false
bibtex_author: Stojanovic, Stefan and Donhauser, Konstantin and Yang, Fanny
author:
- given: Stefan
  family: Stojanovic
- given: Konstantin
  family: Donhauser
- given: Fanny
  family: Yang
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/stojanovic24a/stojanovic24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
