---
title: A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks
abstract: 'We study the phenomenon of in-context learning (ICL) exhibited by large
  language models, where they can adapt to a new learning task, given a handful of
  labeled examples, without any explicit parameter optimization. Our goal is to explain
  how a pre-trained transformer model is able to perform ICL under reasonable assumptions
  on the pre-training process and the downstream tasks. We posit a mechanism whereby
  a transformer can achieve the following: (a) receive an i.i.d. sequence of examples
  which have been converted into a prompt using potentially-ambiguous delimiters,
  (b) correctly segment the prompt into examples and labels, (c) infer from the data
  a sparse linear regressor hypothesis, and finally (d) apply this hypothesis on the
  given test example and return a predicted label. We establish that this entire procedure
  is implementable using the transformer mechanism, and we give sample complexity
  guarantees for this learning framework. Our empirical findings validate the challenge
  of segmentation, and we show a correspondence between our posited mechanisms and
  observed attention maps for step (c).'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: abernethy24a
month: 0
tex_title: A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval
  Tasks
firstpage: 3
lastpage: 46
page: 3-46
order: 3
cycles: false
bibtex_author: Abernethy, Jacob and Agarwal, Alekh and Marinov, Teodor Vanislavov
  and Warmuth, Manfred K.
author:
- given: Jacob
  family: Abernethy
- given: Alekh
  family: Agarwal
- given: Teodor Vanislavov
  family: Marinov
- given: Manfred K.
  family: Warmuth
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/abernethy24a/abernethy24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
