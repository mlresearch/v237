---
title: The Impossibility of Parallelizing Boosting
abstract: The aim of boosting is to convert a sequence of weak learners into a strong
  learner. At their heart, these methods are fully sequential. In this paper, we investigate
  the possibility of parallelizing boosting. Our main contribution is a strong negative
  result, implying that significant parallelization of boosting requires an exponential
  blow-up in the total computing resources needed for training.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: karbasi24a
month: 0
tex_title: The Impossibility of Parallelizing Boosting
firstpage: 635
lastpage: 653
page: 635-653
order: 635
cycles: false
bibtex_author: Karbasi, Amin and Green Larsen, Kasper
author:
- given: Amin
  family: Karbasi
- given: Kasper
  family: Green Larsen
date: 2024-03-15
address:
container-title: Proceedings of The 35th International Conference on Algorithmic Learning
  Theory
volume: '237'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 3
  - 15
pdf: https://proceedings.mlr.press/v237/karbasi24a/karbasi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
